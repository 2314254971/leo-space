## 大模型下载

modelscope

huggingface（镜像hfmirror）

### 模型文件解读

主流大模型权重格式为safetensor和gguf

#### 详细对比

| 特性                              | **SafeTensor**                                    | **GGUF**                                                     |
| --------------------------------- | ------------------------------------------------- | ------------------------------------------------------------ |
| **主要用途**                      | 安全加载权重到 PyTorch/TensorFlow/HF Transformers | 在 llama.cpp 中进行 CPU/GPU 高效推理                         |
| **是否包含模型结构？**            | ❌ 仅权重（需配合 `config.json`）                  | ✅ **包含完整的模型拓扑描述**（如层数、头数、隐藏维度、是否使用 RoPE、激活函数类型等） |
| **是否支持量化？**                | ❌ 原生只支持 FP16/FP32/BF16 等标准 dtype          | ✅ **原生支持多种量化格式**（如 Q4_K_M, Q5_K_S, Q8_0 等），量化信息直接写入文件头 |
| **是否包含分词器（tokenizer）？** | ❌ 不包含（需单独 `tokenizer.json`）               | ✅ **通常内嵌 BPE 或 SentencePiece 分词器信息**（vocab、merges、特殊 token 等） |
| **文件结构**                      | 简单：header（metadata）+ 张量数据（可内存映射）  | 复杂：自定义二进制格式，含 metadata、张量定义、量化参数、分词表等 |
| **依赖环境**                      | 需 Python + `safetensors` 库                      | 可纯 C/C++ 加载，无需 Python                                 |
| **典型使用场景**                  | Hugging Face 模型分发、训练恢复、安全部署         | 在手机、PC、服务器上用 llama.cpp / Ollama / LM Studio 运行大模型 |

#### safetensor

safetensor文件仅包含权重信息，不包含模型结构。（因此一定存在一个模型结构定义文件或者模型实现，这一点和是深度学习框架一致的）

常见的大模型结构配置都写在了**config.json**文件中

例如：

```json
{
  "architectures": ["LlamaForCausalLM"],
  "hidden_size": 4096,
  "intermediate_size": 11008,
  "num_hidden_layers": 32,
  "num_attention_heads": 32,
  "num_key_value_heads": 32,
  "rms_norm_eps": 1e-05,
  "vocab_size": 32000,
  "max_position_embeddings": 2048,
  "model_type": "llama"
}
```

各字段含义（与模型结构直接相关）：

| 字段                      | 作用                                               |
| ------------------------- | -------------------------------------------------- |
| `model_type`              | 决定使用哪个模型类（如 `"llama"` → `LlamaModel`）  |
| `architectures`           | 指明顶层模型类（用于 `AutoModel` 自动选择）        |
| `num_hidden_layers`       | 决定 Transformer 层堆叠数量（即第 0 层到第 31 层） |
| `hidden_size`             | 每个 token 的向量维度（如 4096）                   |
| `num_attention_heads`     | 多头注意力的头数                                   |
| `intermediate_size`       | FFN 层的中间维度                                   |
| `vocab_size`              | 词表大小，决定 embedding 层输出维度                |
| `max_position_embeddings` | 支持的最大序列长度                                 |

> 🔍 实际上，**所有层都是同构的**（除了可能的特殊层如 embedding 或 head），所以只需指定 `num_hidden_layers`，框架就会循环创建 N 个相同的 Transformer 层。



architectures参数指定了模型实现类，当然**前提是transformers库内部包含这个实现类**，像qwen架构（[通义千问3-VL-8B-Instruct · 模型库](https://www.modelscope.cn/models/Qwen/Qwen3-VL-8B-Instruct/summary)），基本上库内直接有，所以就不需要写类的定义。

```python
from transformers import AutoConfig
config = AutoConfig.from_pretrained("path/to/config.json")
```

如果是一些自定义的模型架构，则需要进行注册

以deepseek为例：
[DeepSeek-OCR-8bit · 模型库](https://www.modelscope.cn/models/mlx-community/DeepSeek-OCR-8bit/files)

在config.json中指定了架构：

"architectures": [
        "DeepseekOCRForCausalLM"
 ],

但是该架构属于自定义架构，因此需要自行注册：

大致代码如下：

```python
# my_model.py
from transformers import PretrainedConfig, PreTrainedModel
import torch.nn as nn

class MyModelConfig(PretrainedConfig):
    model_type = "mymodel"  # ← 这个字符串必须唯一，且与 config.json 中一致

    def __init__(self, hidden_size=768, num_layers=12, **kwargs):
        super().__init__(**kwargs)
        self.hidden_size = hidden_size
        self.num_layers = num_layers

class MyModel(PreTrainedModel):
    config_class = MyModelConfig  # ← 关联配置类

    def __init__(self, config):
        super().__init__(config)
        self.layers = nn.ModuleList([
            nn.Linear(config.hidden_size, config.hidden_size)
            for _ in range(config.num_layers)
        ])

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
```

**因此对于自定义架构基本都会提供py代码**



### 其他配置







## 大模型部署

个人学习研究使用推荐ollama，占用显存小，底层基于llamacpp，性能较好



生产级推荐

vllm（无界面，可命令行或者代码使用，有点高并发），

xinfrence（集成多种框架，自带界面，图省事更推荐这个），

sglang



## 大模型-->智能体

个人理解：智能体=能调用工具的大模型

[详解LLM大模型是如何理解并使用 tools ？_llm tools-CSDN博客](https://blog.csdn.net/2401_82469710/article/details/139984847)





## 上下文

### 上下文限制原理：

大模型上下文长度限制的本质原因涉及**模型结构、计算资源、训练策略**三者的综合约束，而不仅仅是单一因素。

#### **1. 模型结构约束**

- **位置编码的容量限制**：
  - **绝对位置编码**（如原始Transformer）：预定义最大位置数（如512），超出后无法生成有效位置向量，导致模型难以理解更远距离的token关系。
  - **相对位置编码**（如RoPE、ALiBi）：虽支持外推，但训练时使用的最大长度（如2048）决定了模型对长距离关系的建模能力。超出训练长度时，位置关系的计算可能失效（例如RoPE外推时出现注意力分数混乱）。
- **注意力机制的计算复杂度**：
  - 自注意力计算的时间和内存复杂度为 O(n2)*O*(*n*2)（n为序列长度）。即使使用优化方法（如FlashAttention），实际工程中仍会设定最大长度以控制资源消耗。

#### **2. 训练数据与策略**

- 长序列训练成本高：训练时若要求模型处理超长文本（如64K tokens），需要更大的显存和更长的训练时间，这对硬件和数据集构造提出了极高要求。
- 长文本数据的稀疏性：实际语料中长程依赖（如跨越数千token的指代关系）较少，模型难以充分学习长距离依赖的规律。

#### **3. 工程实现限制**

- **显存占用**：GPU显存容量限制了单次处理的最大token数。例如，处理1K tokens可能需要10GB显存，2K tokens则需约40GB（复杂度平方增长）。
- **推理速度**：长序列会显著降低生成速度，影响用户体验。





### 大模型的记忆本质：

#### **一、大模型的本质：无状态的文本生成器**

1. **模型架构的限制**

   - Transformer模型的每一次推理都是**独立计算**，模型参数在推理过程中**不会改变**（权重固定）。
   - 模型输出仅依赖当前输入的上下文（即输入的token序列），不会主动存储或回忆历史对话。

2. **对话中的“记忆”是上下文拼接的假象**

   - 当用户与大模型进行多轮对话时，系统需要**人为将历史对话和当前问题拼接成完整的输入序列**，例如：

     复制

     ```
     [用户: 你好！]
     [AI: 你好，有什么可以帮您？]
     [用户: 什么是量子计算？] ← 当前输入
     ```

     实际输入模型的完整上下文是：

     复制

     ```
     用户: 你好！\nAI: 你好，有什么可以帮您？\n用户: 什么是量子计算？
     ```

   - 模型仅根据这个拼接后的长文本生成回复，而非“记住”之前的对话。

------

#### **二、为什么模型需要依赖外部拼接上下文？**

1. **注意力机制的范围限制**
   Transformer的自注意力机制仅能处理当前输入序列内的token关联。例如，若输入包含历史对话和当前问题，模型可以分析它们之间的关系；但如果未提供历史信息，模型对这些信息“一无所知”。
2. **上下文窗口的硬性限制**
   - 模型的最大上下文长度（如GPT-4的128K tokens）决定了能拼接的历史对话量。
   - 超出窗口长度的历史内容会被丢弃或截断（通常保留最近的对话），模型无法访问更早的信息。

------

#### **三、模型“记忆”能力的实际表现**

1. **短期记忆（Short-term Memory）**

   - 通过上下文拼接，模型可以暂时记住当前对话中的信息。例如：

     复制

     ```
     用户: 我叫张三。
     AI: 你好张三！
     用户: 我刚才说了我的名字吗？
     AI: 您刚才说您叫张三。
     ```

   - 这种“记忆”完全依赖输入中包含“我叫张三”的历史记录。

2. **长期记忆的缺失（No Long-term Memory）**

   - 如果用户关闭对话后重新开始，模型无法记住之前的交互（除非应用层额外存储历史并重新拼接）。
   - 模型无法像人类一样主动回忆未在上下文中出现的信息。

------

#### **四、与大模型记忆相关的技术探索**

尽管模型本身无记忆能力，但研究者正在尝试扩展这一限制：

1. **检索增强生成（Retrieval-Augmented Generation, RAG）**

   - 通过外部数据库检索相关信息，动态拼接到输入上下文中，模拟长期记忆。例如：

     复制

     ```
     用户: 量子计算有什么新进展？
     → 系统从数据库中检索最新论文摘要 → 拼接后输入模型。
     ```

2. **记忆网络（Memory Networks）**

   - 显式设计可读写的外部记忆模块（如向量数据库），但这类方法尚未在主流大模型中广泛应用。

3. **参数化记忆（Parameterized Memory）**

   - 在训练阶段将知识固化到模型参数中（如训练数据包含百科全书），但这属于静态知识而非动态记忆。

------

#### **五、示例：对话中的“记忆”如何实现**

假设模型支持4K上下文，以下是两轮对话的输入变化：

- **第一轮输入**

  复制

  ```
  用户: 巴黎是哪个国家的首都？
  AI: 巴黎是法国的首都。
  ```

- **第二轮输入（用户未提供历史）**

  复制

  ```
  用户: 它的人口多少？
  → 模型因缺少上下文，无法回答“它”指代什么。
  ```

- **第二轮输入（拼接历史后）**

  复制

  ```
  用户: 巴黎是哪个国家的首都？
  AI: 巴黎是法国的首都。
  用户: 它的人口多少？ ← 实际输入模型的完整上下文
  → AI回复: 巴黎的人口约220万。
  ```

------

#### **六、总结**

- **大模型没有主动记忆能力**，其“记忆”完全依赖外部输入的上下文。
- 多轮对话的连贯性由**人为拼接历史信息**实现，受上下文窗口长度限制。
- 长期记忆需依赖外部技术（如RAG），模型自身无法突破这一限制。



相关问题：

- **分块输入能否串联**？
  - **不能自动串联**：模型每次推理独立进行，无记忆能力。
  - **可人工辅助串联**：通过摘要重叠、检索增强等方法，以外接方式模拟长上下文处理，但需额外工程设计。

总之，若没有超过最大上下文，可以记住，超过就没办法了，只能采取其他技巧



### 超长文本处理方案