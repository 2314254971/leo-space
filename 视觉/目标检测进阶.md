





# 小目标

[yolov5s增加小目标检测层，同时删除大目标的检测层，适用于全是小目标的数据集-CSDN博客](https://blog.csdn.net/weixin_56828987/article/details/136760104)

[【Yolov5】Yolov5添加检测层，四层结构对小目标、密集场景更友好_yolov5增加检测层-CSDN博客](https://blog.csdn.net/weixin_50006912/article/details/129122501)

YOLOv3/v5/v8 引入 PAFPN/Path Aggregation → 小目标提升，但**在密集小目标场景（如遥感、细胞检测）仍逊于 Faster R-CNN + FPN**。？？？

V8如果小目标或者大目标怎么调整？，regmax吗？？



## 标签分配

图片来源：[演示 - 飞书云文档](https://mukdrpaizj.feishu.cn/docx/JiF3dsx9eonwiHxlZCScznr2nke)

![](D:\leo space\视觉\asset\标签分配1.png)

简单理解：在给定一幅图像及其目标 gt bbox的情况下，为每个目标 gt bbox 选择恰当的特征图point进行学习预测的过程就是标签分配。



rule-based

基于规则



auto-assign







# 训练技巧

### YOLO其他超参数细节

warmup  初始nw各batch内学习率逐步上升，增大学习效率

accumulate梯度累计到一定程度再进行优化，以小batch模拟大batch效果



```yaml
# 这三个参数设置的是损失函数加权
box: 0.05  # box loss gain
cls: 0.3  # cls loss gain
obj: 0.7  # obj loss gain (scale with pixels)

# nms时的iou阈值
iou_t: 0.20  # IoU training threshold 
```



### 类别不平衡/样本不平衡

#### 解决方案一：调整权重

cls_pw: 1.0  # cls BCELoss positive_weight 

obj_pw: 1.0  # obj BCELoss positive_weight  

平衡正负样本损失 （调整pos_weight）

样本损失加权（调整weight）



原理：

BCE(𝑦,𝑝) = −[ 𝑦·log𝑝 + (1−𝑦)·log(1−𝑝) ]

1. 为什么“第一项”对应正样本？

- 𝑦=1 时，第二项 (1−𝑦)=0，整个损失只剩下 −log𝑝。
  也就是说，**只要标签是 1，网络就必须把 𝑝 推接近 1，否则 −log𝑝 会爆炸**。
- 𝑦=0 时，第一项 𝑦=0，只剩 −log(1−𝑝)，要求 𝑝 接近 0。
  因此 **带 𝑦 的那一项就是“正样本”项**，带 (1−𝑦) 的是“负样本”项。



BCEWithLogitsLoss 内部实现等价于

```
loss_elem = − pos_weight·y·log σ(z) − (1−y)·log(1−σ(z))
```

随后再把 `loss_elem` 与外部 `weight` 相乘：

```
loss_elem = loss_elem * weight        # 如果提供了 weight
```

最后做 `reduction='mean' or 'sum'`。



损失计算

上述损失函数仅仅是单个类别的损失，BCE损失是所有类别损失执行reduce操作（求和/平均值）

举个完整的小例子

图片里“**同时**有猫、无狗”，模型输出两个独立概率。

|                  | 猫    | 狗    |
| :--------------- | :---- | :---- |
| 真实 y           | 1     | 0     |
| 网络原始 logit z | 2.0   | −1.0  |
| sigmoid 后 p     | 0.881 | 0.269 |

BCE 计算：
L_cat = − [1·log(0.881) + 0·log(0.119)] ≈ 0.127
L_dog = − [0·log(0.269) + 1·log(0.731)] ≈ 0.313
总损失 L_BCE = 0.127 + 0.313 = **0.440**



batch=2，每张图同时做“猫/狗”两个二分类，共 4 个 loss 元素：

| 样本 | 任务 | 逐元素 loss |
| :--- | :--- | :---------- |
| 图-1 | 猫   | 0.20        |
| 图-1 | 狗   | 0.15        |
| 图-2 | 猫   | 0.30        |
| 图-2 | 狗   | 0.25        |

情况 A：给每张图一个权重
weight = [3.0, 1.0]          # shape=(2,1) 或 (2,2) 广播后
逐点乘完：
[0.20, 0.15] * 3.0  → [0.60, 0.45]
[0.30, 0.25] * 1.0  → [0.30, 0.25]

若 reduction='mean'，总 loss = (0.60+0.45+0.30+0.25) / (3+3+1+1) = 1.6/8 = 0.20

情况 B：给每个标签单独权重
weight = [[2, 1], [1, 3]]    # shape=(2,2)
同理先逐点乘，再按指定 reduction 合并。



参考：https://blog.csdn.net/Zuobf_bk/article/details/135550345 

https://docs.pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss

#### 解决方案二：使用focalloss





### 超参数动态调度

学习率调整：

线性下降

余弦退火

onecyle：

学习率周期变化，学习率从小到大再变回小，周期往复

```python
# 下限为y1，上限为y2，往复consine余弦变化（小→大→小）
def one_cycle(y1=0.0, y2=1.0, steps=100):
    # lambda function for sinusoidal ramp from y1 to y2
    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1
```



### 正则化：



一、数学原理（核心：L2 正则化）

Weight decay 本质上是 **在损失函数中添加一个关于模型权重的 L2 范数惩罚项**，即：

$$L_{total}=L_{data}+λ*\frac{1}{2}*\sum\limits_i w_i^2$$

其中：

- Ldata 是原始任务损失（如分类交叉熵、检测的 box loss）；
- $\sum\limits_i w_i^2$ 是所有权重参数的平方和（L2 范数的平方）；
- *λ* 就是 `weight_decay`（超参数，如 `1e-4`, `5e-4`）；
- 因子 `1/2` 是为了求导后简洁（不影响本质）。

👉 对权重$w_i$ 求梯度并更新时（以 SGD 为例）：

$$ \frac{\partial L_{total}}{\partial w_i} = \frac{\partial L_{data}}{\partial w_i} +λ*w_i  $$ 

更新公式变为：

$$ w_i ← w_i - \eta (\frac{\partial L_{total}}{\partial w_i} + \lambda*w_i )= w_i(1-\eta\lambda)\frac{\partial L_{data}}{\partial w_i}$$

👉 **关键观察**：
每一步更新前，权重先被乘上一个小于 1 的因子 (1−*η λ*) —— 这就是 **“decay”（衰减）** 的直接来源！
所以叫 *weight decay*，非常形象。

> 💡 注：虽然叫 “decay”，但它**不是学习率衰减（lr decay）**，而是对权重值本身的指数级收缩趋势。 

二、直观理解：为什么需要它？

✅ 作用 1：防止过拟合（核心目的）

- 大权重 → 模型对输入微小变化极度敏感 → 容易记住噪声（过拟合）；
- 加 L2 惩罚后 → 优化器倾向于选**更小、更平滑的权重** → 模型更“保守” → 泛化更好。

📌 类比：

> 就像考试划重点——老师说“别钻牛角尖（大权重），抓住主干知识（中小权重）就行”。 

✅ 作用 2：提升训练稳定性

- 极大权重可能导致梯度爆炸、数值溢出；
- Weight decay 起到“软约束”，让参数保持在合理范围。

✅ 作用 3：隐式控制模型复杂度

- 大 λ ≈ 强正则 ≈ 模型偏向简单（接近线性）；
- 小 λ ≈ 弱正则 ≈ 模型自由度更高（易拟合复杂模式）。

```python
import torch
import torch.nn as nn

model = nn.Linear(2, 1)
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.5)

x = torch.randn(1, 2)
y = torch.randn(1, 1)

loss = nn.MSELoss()(model(x), y)
loss.backward()
print("Before step: w =", model.weight.data.clone())
optimizer.step()
print("After step:  w =", model.weight.data)  # 会发现权重被 (1 - lr*wd)=0.95 缩小了！
```

注：

yolo中仅对weight进行衰减，其他不衰减，比如bias和batchnorm（这两个正则化效果不好，正则化是把参数向0靠拢）



