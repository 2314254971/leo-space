## 大模型-->智能体

个人理解：智能体=能调用工具的大模型

[详解LLM大模型是如何理解并使用 tools ？_llm tools-CSDN博客](https://blog.csdn.net/2401_82469710/article/details/139984847)





## 上下文

### 上下文限制原理：

大模型上下文长度限制的本质原因涉及**模型结构、计算资源、训练策略**三者的综合约束，而不仅仅是单一因素。

#### **1. 模型结构约束**

- **位置编码的容量限制**：
  - **绝对位置编码**（如原始Transformer）：预定义最大位置数（如512），超出后无法生成有效位置向量，导致模型难以理解更远距离的token关系。
  - **相对位置编码**（如RoPE、ALiBi）：虽支持外推，但训练时使用的最大长度（如2048）决定了模型对长距离关系的建模能力。超出训练长度时，位置关系的计算可能失效（例如RoPE外推时出现注意力分数混乱）。
- **注意力机制的计算复杂度**：
  - 自注意力计算的时间和内存复杂度为 O(n2)*O*(*n*2)（n为序列长度）。即使使用优化方法（如FlashAttention），实际工程中仍会设定最大长度以控制资源消耗。

#### **2. 训练数据与策略**

- 长序列训练成本高：训练时若要求模型处理超长文本（如64K tokens），需要更大的显存和更长的训练时间，这对硬件和数据集构造提出了极高要求。
- 长文本数据的稀疏性：实际语料中长程依赖（如跨越数千token的指代关系）较少，模型难以充分学习长距离依赖的规律。

#### **3. 工程实现限制**

- **显存占用**：GPU显存容量限制了单次处理的最大token数。例如，处理1K tokens可能需要10GB显存，2K tokens则需约40GB（复杂度平方增长）。
- **推理速度**：长序列会显著降低生成速度，影响用户体验。





### 大模型的记忆本质：

#### **一、大模型的本质：无状态的文本生成器**

1. **模型架构的限制**

   - Transformer模型的每一次推理都是**独立计算**，模型参数在推理过程中**不会改变**（权重固定）。
   - 模型输出仅依赖当前输入的上下文（即输入的token序列），不会主动存储或回忆历史对话。

2. **对话中的“记忆”是上下文拼接的假象**

   - 当用户与大模型进行多轮对话时，系统需要**人为将历史对话和当前问题拼接成完整的输入序列**，例如：

     复制

     ```
     [用户: 你好！]
     [AI: 你好，有什么可以帮您？]
     [用户: 什么是量子计算？] ← 当前输入
     ```

     实际输入模型的完整上下文是：

     复制

     ```
     用户: 你好！\nAI: 你好，有什么可以帮您？\n用户: 什么是量子计算？
     ```

   - 模型仅根据这个拼接后的长文本生成回复，而非“记住”之前的对话。

------

#### **二、为什么模型需要依赖外部拼接上下文？**

1. **注意力机制的范围限制**
   Transformer的自注意力机制仅能处理当前输入序列内的token关联。例如，若输入包含历史对话和当前问题，模型可以分析它们之间的关系；但如果未提供历史信息，模型对这些信息“一无所知”。
2. **上下文窗口的硬性限制**
   - 模型的最大上下文长度（如GPT-4的128K tokens）决定了能拼接的历史对话量。
   - 超出窗口长度的历史内容会被丢弃或截断（通常保留最近的对话），模型无法访问更早的信息。

------

#### **三、模型“记忆”能力的实际表现**

1. **短期记忆（Short-term Memory）**

   - 通过上下文拼接，模型可以暂时记住当前对话中的信息。例如：

     复制

     ```
     用户: 我叫张三。
     AI: 你好张三！
     用户: 我刚才说了我的名字吗？
     AI: 您刚才说您叫张三。
     ```

   - 这种“记忆”完全依赖输入中包含“我叫张三”的历史记录。

2. **长期记忆的缺失（No Long-term Memory）**

   - 如果用户关闭对话后重新开始，模型无法记住之前的交互（除非应用层额外存储历史并重新拼接）。
   - 模型无法像人类一样主动回忆未在上下文中出现的信息。

------

#### **四、与大模型记忆相关的技术探索**

尽管模型本身无记忆能力，但研究者正在尝试扩展这一限制：

1. **检索增强生成（Retrieval-Augmented Generation, RAG）**

   - 通过外部数据库检索相关信息，动态拼接到输入上下文中，模拟长期记忆。例如：

     复制

     ```
     用户: 量子计算有什么新进展？
     → 系统从数据库中检索最新论文摘要 → 拼接后输入模型。
     ```

2. **记忆网络（Memory Networks）**

   - 显式设计可读写的外部记忆模块（如向量数据库），但这类方法尚未在主流大模型中广泛应用。

3. **参数化记忆（Parameterized Memory）**

   - 在训练阶段将知识固化到模型参数中（如训练数据包含百科全书），但这属于静态知识而非动态记忆。

------

#### **五、示例：对话中的“记忆”如何实现**

假设模型支持4K上下文，以下是两轮对话的输入变化：

- **第一轮输入**

  复制

  ```
  用户: 巴黎是哪个国家的首都？
  AI: 巴黎是法国的首都。
  ```

- **第二轮输入（用户未提供历史）**

  复制

  ```
  用户: 它的人口多少？
  → 模型因缺少上下文，无法回答“它”指代什么。
  ```

- **第二轮输入（拼接历史后）**

  复制

  ```
  用户: 巴黎是哪个国家的首都？
  AI: 巴黎是法国的首都。
  用户: 它的人口多少？ ← 实际输入模型的完整上下文
  → AI回复: 巴黎的人口约220万。
  ```

------

#### **六、总结**

- **大模型没有主动记忆能力**，其“记忆”完全依赖外部输入的上下文。
- 多轮对话的连贯性由**人为拼接历史信息**实现，受上下文窗口长度限制。
- 长期记忆需依赖外部技术（如RAG），模型自身无法突破这一限制。



相关问题：

- **分块输入能否串联**？
  - **不能自动串联**：模型每次推理独立进行，无记忆能力。
  - **可人工辅助串联**：通过摘要重叠、检索增强等方法，以外接方式模拟长上下文处理，但需额外工程设计。

总之，若没有超过最大上下文，可以记住，超过就没办法了，只能采取其他技巧



### 超长文本处理方案