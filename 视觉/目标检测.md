# 标签标注

标注工具labelme，官网：https://github.com/wkentaro/labelme

安装

pip（推荐）

```sh
#最好先conda先创建环境然后再安装，防止包冲突，推荐python版本为3.10
# 安装
pip install labelme

# 运行
labelme
```



打包为可执行文件：（不推荐）

```sh
LABELME_PATH=./labelme
OSAM_PATH=$(python -c 'import os, osam; print(os.path.dirname(osam.__file__))')
pyinstaller labelme/labelme/__main__.py \
  --name=Labelme \
  --windowed \
  --noconfirm \
  --specpath=build \
  --add-data=$(OSAM_PATH)/_models/yoloworld/clip/bpe_simple_vocab_16e6.txt.gz:osam/_models/yoloworld/clip \
  --add-data=$(LABELME_PATH)/config/default_config.yaml:labelme/config \
  --add-data=$(LABELME_PATH)/icons/*:labelme/icons \
  --add-data=$(LABELME_PATH)/translate/*:translate \
  --icon=$(LABELME_PATH)/icons/icon-256.png \
  --onedir
```

## 标注结果

```
{
    "version": "5.0.1",                  // LabelMe软件的版本号
    "flags": {},                        // 用户定义的标志或注释
    "shapes": [                         // 图像中所有标注的集合
        {
            "label": "car",             // 标注对象的类别标签
            "points": [[10, 20], [30, 40]], // 多边形顶点坐标，对于矩形是两对角点
            "group_id": null,           // 可选的组ID，用于关联多个形状
            "shape_type": "rectangle",  // 形状类型：rectangle, polygon, points, line, etc.
            "flags": {}                 // 形状特定的标志
        },
        // 更多的形状...
    ],
    "imagePath": "path/to/image.jpg",   // 原始图像的相对路径
    "imageData": null,                  // 图像的Base64编码，如果图像文件较大，通常为空
    "imageHeight": 480,                 // 图像的高度
    "imageWidth": 640                   // 图像的宽度
}
```



## 注意事项

1.标签标注前，尽量先写标签文件节省打标签时间

预先把数据集的类别写入txt

例如新建label.txt

```
bus
car
motorcycle
truck
```

在labelme启动时加入参数指定label文件位置

例如

```sh
labelme --labels label.txt
```

2.标签文件默认生成在图片相同目录下，如果需要修改目录可以点击”更改输出目录“

3.取消勾选”同时保存图像数据“，这一项默认勾选，勾选后会把图像进行base64编码保存至json，没有必要

4.labelme默认读取 ”changeOutputDir”变量所指的目录来加载标签 即“更改输出目录”所指向的目录（https://github.com/wkentaro/labelme/blob/e248eae029e905aeddcb96b4be17e2c643b1e18f/labelme/app.py#L303C9-L303C23）



## 其他用法

作为库使用

[【图像分割】LabelMe基本使用/标注标签格式转换及可视化-腾讯云开发者社区-腾讯云](https://cloud.tencent.com/developer/article/2274889)



```python
import labelme

```



# 数据集类型

## 参考

[(29 封私信 / 80 条消息) 【目标检测】yolo的三种数据集格式 - 知乎](https://zhuanlan.zhihu.com/p/525950939)

## YOLO（txt）

![img](https://pica.zhimg.com/v2-d29ecbc37b52a305e5791d8d32e61db6_1440w.jpg)

图片和标签分别两个文件夹中，并且自己分类，将图片与标签按照一定的比例区分成训练集和验证集。

![img](https://pica.zhimg.com/v2-3b05de815cf88c3249395c4531101c8a_1440w.jpg)

第一列为目标类别，后面四个数字为**[x_center, y_center, w, h]**，可以看到都是小于1的数字，是因为对应的整张图片的比例，所以就算图像被拉伸放缩，这种txt格式的标签也可以找到相应的目标。

yaml配置

![img](https://pic1.zhimg.com/v2-dee90a1bae3d230d43b59978c43e8e72_1440w.jpg)





## Pascal VOC（xml）

![img](https://pic3.zhimg.com/v2-9b5df21cabfed7da427d113034133230_1440w.jpg)

① JPEGImages文件夹：数据集图片

② Annotations文件夹：标注，与图片对应的xml文件

③ ImageSets/Main文件夹：将数据集分为训练集和验证集，因此产生的train.txt和val.txt

区分训练集与验证集的操作集在了Imagesets中

![img](https://pica.zhimg.com/v2-2ead680d000aef85c28b6249aa224bb0_1440w.jpg)

## COCO（json）



## 格式转换

### txt2xml

```python
from xml.dom.minidom import Document
import os
import cv2


# def makexml(txtPath, xmlPath, picPath):  # txt所在文件夹路径，xml文件保存路径，图片所在文件夹路径
def makexml(picPath, txtPath, xmlPath):  # txt所在文件夹路径，xml文件保存路径，图片所在文件夹路径
    """此函数用于将yolo格式txt标注文件转换为voc格式xml标注文件
    在自己的标注图片文件夹下建三个子文件夹，分别命名为picture、txt、xml
    """
    dic = {'0': "bus",  # 创建字典用来对类型进行转换
           '1': "car",  # 此处的字典要与自己的classes.txt文件中的类对应，且顺序要一致
           '2': "truck",
           }
    files = os.listdir(txtPath)
    for i, name in enumerate(files):
        xmlBuilder = Document()
        annotation = xmlBuilder.createElement("annotation")  # 创建annotation标签
        xmlBuilder.appendChild(annotation)
        txtFile = open(txtPath + name)
        txtList = txtFile.readlines()
        img = cv2.imread(picPath + name[0:-4] + ".jpg")
        Pheight, Pwidth, Pdepth = img.shape

        folder = xmlBuilder.createElement("folder")  # folder标签
        foldercontent = xmlBuilder.createTextNode("driving_annotation_dataset")
        folder.appendChild(foldercontent)
        annotation.appendChild(folder)  # folder标签结束

        filename = xmlBuilder.createElement("filename")  # filename标签
        filenamecontent = xmlBuilder.createTextNode(name[0:-4] + ".jpg")
        filename.appendChild(filenamecontent)
        annotation.appendChild(filename)  # filename标签结束

        size = xmlBuilder.createElement("size")  # size标签
        width = xmlBuilder.createElement("width")  # size子标签width
        widthcontent = xmlBuilder.createTextNode(str(Pwidth))
        width.appendChild(widthcontent)
        size.appendChild(width)  # size子标签width结束

        height = xmlBuilder.createElement("height")  # size子标签height
        heightcontent = xmlBuilder.createTextNode(str(Pheight))
        height.appendChild(heightcontent)
        size.appendChild(height)  # size子标签height结束

        depth = xmlBuilder.createElement("depth")  # size子标签depth
        depthcontent = xmlBuilder.createTextNode(str(Pdepth))
        depth.appendChild(depthcontent)
        size.appendChild(depth)  # size子标签depth结束

        annotation.appendChild(size)  # size标签结束

        for j in txtList:
            oneline = j.strip().split(" ")
            object = xmlBuilder.createElement("object")  # object 标签
            picname = xmlBuilder.createElement("name")  # name标签
            namecontent = xmlBuilder.createTextNode(dic[oneline[0]])
            picname.appendChild(namecontent)
            object.appendChild(picname)  # name标签结束

            pose = xmlBuilder.createElement("pose")  # pose标签
            posecontent = xmlBuilder.createTextNode("Unspecified")
            pose.appendChild(posecontent)
            object.appendChild(pose)  # pose标签结束

            truncated = xmlBuilder.createElement("truncated")  # truncated标签
            truncatedContent = xmlBuilder.createTextNode("0")
            truncated.appendChild(truncatedContent)
            object.appendChild(truncated)  # truncated标签结束

            difficult = xmlBuilder.createElement("difficult")  # difficult标签
            difficultcontent = xmlBuilder.createTextNode("0")
            difficult.appendChild(difficultcontent)
            object.appendChild(difficult)  # difficult标签结束

            bndbox = xmlBuilder.createElement("bndbox")  # bndbox标签
            xmin = xmlBuilder.createElement("xmin")  # xmin标签
            mathData = int(((float(oneline[1])) * Pwidth + 1) - (float(oneline[3])) * 0.5 * Pwidth)
            xminContent = xmlBuilder.createTextNode(str(mathData))
            xmin.appendChild(xminContent)
            bndbox.appendChild(xmin)  # xmin标签结束

            ymin = xmlBuilder.createElement("ymin")  # ymin标签
            mathData = int(((float(oneline[2])) * Pheight + 1) - (float(oneline[4])) * 0.5 * Pheight)
            yminContent = xmlBuilder.createTextNode(str(mathData))
            ymin.appendChild(yminContent)
            bndbox.appendChild(ymin)  # ymin标签结束

            xmax = xmlBuilder.createElement("xmax")  # xmax标签
            mathData = int(((float(oneline[1])) * Pwidth + 1) + (float(oneline[3])) * 0.5 * Pwidth)
            xmaxContent = xmlBuilder.createTextNode(str(mathData))
            xmax.appendChild(xmaxContent)
            bndbox.appendChild(xmax)  # xmax标签结束

            ymax = xmlBuilder.createElement("ymax")  # ymax标签
            mathData = int(((float(oneline[2])) * Pheight + 1) + (float(oneline[4])) * 0.5 * Pheight)
            ymaxContent = xmlBuilder.createTextNode(str(mathData))
            ymax.appendChild(ymaxContent)
            bndbox.appendChild(ymax)  # ymax标签结束

            object.appendChild(bndbox)  # bndbox标签结束

            annotation.appendChild(object)  # object标签结束

        f = open(xmlPath + name[0:-4] + ".xml", 'w')
        xmlBuilder.writexml(f, indent='\t', newl='\n', addindent='\t', encoding='utf-8')
        f.close()


if __name__ == "__main__":
    picPath = "./yolox/JPEGImages/"  # 图片所在文件夹路径，后面的/一定要带上
    txtPath = "./yolox/YOLO/"  # txt所在文件夹路径，后面的/一定要带上
    xmlPath = "./yolox/Annotations/"  # xml文件保存路径，后面的/一定要带上
    makexml(picPath, txtPath, xmlPath)
```

### json2yolo

```python
import json
import os

category_dict = {'飞机': '1'}  # 类别字典

def json_to_yolo(input_file_path, output_directory):
    data = json.load(open(input_file_path, encoding="utf-8"))  # 读取带有中文的文件
    image_width = data["imageWidth"]  # 获取json文件里图片的宽度
    image_height = data["imageHeight"]  # 获取json文件里图片的高度
    yolo_format_content = ''

    for shape in data["shapes"]:
        # 归一化坐标点，并计算中心点(cx, cy)、宽度和高度
        [[x1, y1], [x2, y2]] = shape['points']
        x1, x2 = x1 / image_width, x2 / image_width
        y1, y2 = y1 / image_height, y2 / image_height
        cx = (x1 + x2) / 2
        cy = (y1 + y2) / 2
        width = abs(x2 - x1)
        height = abs(y2 - y1)

        # 将数据组装成YOLO格式
        line = "%s %.4f %.4f %.4f %.4f\n" % (category_dict[shape['label']], cx, cy, width, height)  # 生成txt文件里每行的内容
        yolo_format_content += line

    # 生成txt文件的相应文件路径
    output_file_path = os.path.join(output_directory, os.path.basename(input_file_path).replace('json', 'txt'))
    with open(output_file_path, 'w', encoding='utf-8') as file_handle:
        file_handle.write(yolo_format_content)


input_directory = "E:/DataSet/test/labels/"
output_directory = "E:/DataSet/test/labels-yolo/"

file_list = os.listdir(input_directory)
json_file_list = [file for file in file_list if file.endswith(".json")]  # 获取所有json文件的路径

for json_file in json_file_list:
    json_to_yolo(os.path.join(input_directory, json_file), output_directory)
```

### voc2yolo

```py
import xml.etree.ElementTree as ET
import os
from os import getcwd
 
sets = ['train', 'val', 'test']
classes = ['Power-Pole'] # 根据标签名称填写类别
abs_path = os.getcwd()
print(abs_path)
 
 
def convert(size, box):
    dw = 1. / (size[0])
    dh = 1. / (size[1])
    x = (box[0] + box[1]) / 2.0 - 1
    y = (box[2] + box[3]) / 2.0 - 1
    w = box[1] - box[0]
    h = box[3] - box[2]
    x = x * dw
    w = w * dw
    y = y * dh
    h = h * dh
    return x, y, w, h
 
 
def convert_annotation(image_id):
    in_file = open('data/Annotations/%s.xml' % (image_id), encoding='UTF-8')
    out_file = open('data/labels/%s.txt' % (image_id), 'w')
    tree = ET.parse(in_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)
    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in classes or int(difficult) == 1:
            continue
        cls_id = classes.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text),
             float(xmlbox.find('xmax').text),
             float(xmlbox.find('ymin').text),
             float(xmlbox.find('ymax').text))
        b1, b2, b3, b4 = b
        # 标注越界修正
        if b2 > w:
            b2 = w
        if b4 > h:
            b4 = h
        b = (b1, b2, b3, b4)
        bb = convert((w, h), b)
        out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + '\n')
 
 
wd = getcwd()
for image_set in sets:
    if not os.path.exists('data/labels/'):
        os.makedirs('data/labels/')
    image_ids = open('data/ImageSets/%s.txt' % (image_set)).read().strip().split()
    list_file = open('data/%s.txt' % (image_set), 'w')
    for image_id in image_ids:
        list_file.write(abs_path + '/data/images/%s.jpg\n' % (image_id))
        convert_annotation(image_id)
    list_file.close()
```

labelme2voc

[labelme/examples/bbox_detection/labelme2voc.py at main · wkentaro/labelme](https://github.com/wkentaro/labelme/blob/main/examples/bbox_detection/labelme2voc.py)



## 数据集划分

voc划分

```python
import os
import random
 
trainval_percent = 0.9
train_percent = 0.9
xmlfilepath = 'data/Annotations'
txtsavepath = 'data/ImageSets'
total_xml = os.listdir(xmlfilepath)
 
num = len(total_xml)
list = range(num)
tv = int(num * trainval_percent)
tr = int(tv * train_percent)
trainval = random.sample(list, tv)
train = random.sample(trainval, tr)
 
ftrainval = open('data/ImageSets/trainval.txt', 'w')
ftest = open('data/ImageSets/test.txt', 'w')
ftrain = open('data/ImageSets/train.txt', 'w')
fval = open('data/ImageSets/val.txt', 'w')
 
for i in list:
    name = total_xml[i][:-4] + '\n'
    if i in trainval:
        ftrainval.write(name)
        if i in train:
            ftrain.write(name)
        else:
            fval.write(name)
    else:
        ftest.write(name)
 
ftrainval.close()
ftrain.close()
fval.close()
ftest.close()
```



# 指标

## 前置知识

TP: True Positive, 真正类，将正类预测成正类数。
TN: True Negtive, 真负类，将负类预测成负类数。
FP: False Positive, 假正类，将负类预测成正类。
FN: False Negtive, 假负类，将正类预测成负类。

| positive | negtive |      |
| -------- | ------- | ---- |
| true     | TP      | FP   |
| false    | FN      | TN   |

## Accuracy

准确率

Accuracy =  (TP + TN) / (TP + TN + FP+ FN)  

即预测正确的内容占所有内容的比例

## Precision

精确率 

#在预测的所有正样本中，预测正确的比例 
precision = TP / (TP + FP) 

理解：精确率也被称为查准率，用于衡量检索结果是否精准，即检索结果中，检索到的相关内容占**本次检索结果**的比例

## Recall

召回率

#在所有正样本中，预测为正样本的比例 
recall = TP / (TP + FN)

理解：召回率也被称为查全率，用于衡量检索结果是否全面，遗漏情况，即检索结果中，检索到的相关内容占**总相关内容**的比例

## PR曲线

将预测结果的PR值绘制余坐标轴上，即为PR曲线图（纵轴为P横轴为R）

![articulated_truck.png](https://segmentfault.com/img/bVbDtp3)

## mAP（mean Average Precision）

平均AP，对每个类别计算AP值，然后去平均值

AP = PR曲线图面积  

![在这里插入图片描述](https://i-blog.csdnimg.cn/blog_migrate/a2a754ca23b65dbcc5f3e9b7942ab460.png)

P为precision精确率 

R为recall召回率



曲线面积越大说明AP的值越大，类别的检测精度就越高



### mAP@0.5: 

mean Average Precision（IoU=0.5）
即将IoU设为0.5时，计算每一类的所有图片的AP，然后所有类别求平均，即mAP。

### mAP@.5:.95

（mAP@[.5:.95]）
表示在不同IoU阈值（从0.5到0.95，步长0.05）（0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95）上的平均mAP。



## F1-Score

F1 = 2 * precision * recall /(precision + recall)



## mIOU（Mean Intersection over Union）

miou是语义分割任务中的模型评估标准, 每个类别的iou取平均之后得到miou。（预测结果和GT框之间计算IOU）

iou的计算如下图所示，iou = overlap / union。

![iou_equation.png](https://segmentfault.com/img/bVbDtSF)

相关博客：

[python - 网络评估之mAP和mIOU - 个人文章 - SegmentFault 思否](https://segmentfault.com/a/1190000021783580)





# YOLO



## YOLOV7

### 参考博客

1.[(29 封私信 / 80 条消息) 深入浅出 Yolo 系列之 Yolov7 基础网络结构详解 - 知乎](https://zhuanlan.zhihu.com/p/543743278)

2.[YOLOV7详细解读（一）网络架构解读-阿里云开发者社区](https://developer.aliyun.com/article/1268284)

3.[YOLOV7学习记录之原理+代码介绍_yolov7的介绍和原理-CSDN博客](https://blog.csdn.net/pengxiang1998/article/details/128307956)

4.[(29 封私信 / 80 条消息) yolo系列之yolov7 - 知乎](https://zhuanlan.zhihu.com/p/679005104)

### 模型结构

模型细节：

![详解YOLOV7 网络结构_yolov7网络结构-CSDN博客](https://img-blog.csdnimg.cn/5cb5b3466f61400899b6dd6cf5859c93.png)

![image.png](https://ucc.alicdn.com/pic/developer-ecology/6onfez3baopyo_2690493fa0a24bafb1665d89617ba7ed.png?x-oss-process=image%2Fresize%2Cw_1400%2Cm_lfit%2Fformat%2Cwebp)

![互联网加竞赛 YOLOv7 目标检测网络解读-CSDN博客](https://img-blog.csdnimg.cn/3f046a35f48e43629f83595950ec441c.png)

#### 主干网络backbone

首先 **4** 层卷积层，特征图变为 160 * 160 * 128 大小。CBS** 主要是 **Conv + BN + SiLU** 构成，图中用不同的颜色表示不同的 size 和 stride, 如 (3, 2) 表示卷积核大小为 3 ，步长为 2。

对应配置文件中：

```
[-1, 1, Conv, [32, 3, 1]],  # 0

 [-1, 1, Conv, [64, 3, 2]],  # 1-P1/2      
 [-1, 1, Conv, [64, 3, 1]],
 
 [-1, 1, Conv, [128, 3, 2]],  # 3-P2/4  
```

elan（由多个 CBS 构成）

对应配置文件中：

```
[-1, 1, Conv, [64, 1, 1]],# elan的单独CBS分支
[-2, 1, Conv, [64, 1, 1]],# elan的另一条长分支
[-1, 1, Conv, [64, 3, 1]],
[-1, 1, Conv, [64, 3, 1]],
[-1, 1, Conv, [64, 3, 1]],
[-1, 1, Conv, [64, 3, 1]],
[[-1, -3, -5, -6], 1, Concat, [1]],
[-1, 1, Conv, [256, 1, 1]],  # 11
```

**MP** 层 主要是分为 Maxpool 和 CBS , 其中 MP1 和 MP2 主要是通道数的比变化。

对应配置文件中：

```
[-1, 1, MP, []],
[-1, 1, Conv, [128, 1, 1]],
[-3, 1, Conv, [128, 1, 1]],
[-1, 1, Conv, [128, 3, 2]],
[[-1, -3], 1, Concat, [1]],  # 16-P3/8 
```

 4 个 CBS 后，接入例如一个 ELAN ，然后后面就是三个 MP + ELAN 的输出，对应的就是 C3/C4/C5 的输出，大小分别为 80 * 80 * 512 ， 40 * 40 * 1024， 20 * 20 * 1024。 每一个 MP 由 5 层， ELAN 有 8 层， 所以整个 backbone 的层数为 4 + 8 + 13 * 3 = 51 层， 从 0 开始的话，最后一层就是第 50 层。

#### Head

准确来说Head可以分为两块：FPN-PAN和DetectHead

backbone 最后输出的 32 倍降采样特征图 C5，然后经过 [SPPCSP](https://zhida.zhihu.com/search?content_id=208965058&content_type=Article&match_order=1&q=SPPCSP&zhida_source=entity)，通道数从1024变为512。先按照 top down 和 C4、C3融合，得到 P3、P4 和 P5；再按 bottom-up 去和 P4、P5 做融合。

对于 **pafpn** 输出的 P3、P4 和 P5 ， 经过 [RepConv](https://zhida.zhihu.com/search?content_id=208965058&content_type=Article&match_order=1&q=RepConv&zhida_source=entity) 调整通道数，最后使用 1x1 卷积去预测 objectness（置信度）、class（类别） 和 bbox（预测框） 三部分。

YOLOV7预设了三个尺寸的锚框，因此输出长度为 3*（4（box） + NC + 1（objectness））



RepConv 在训练和推理是有一定的区别。训练时有三个分支的相加输出，部署时会将分支的参数重参数化到主分支上。

![img](https://pic2.zhimg.com/v2-1f39f33049b838cde8996c08343e1673_1440w.jpg)

**重参数化**

现有的经验来看，利用分支，是可以有效增加模型表征学习能力，来提升精度的。

但是做分支的时候，存在一个问题，多分支结构能显著提高模型性能，但是又会最终导致模型在推理时速度变慢且还非常耗内存，这非常不利于实际场景的应用。

速度变慢好理解的，因为结构复杂运算多了，就慢了。

这时就出现模块重参数化解决这个问题。

就是将训练和推理分开，训练是一件事情，推理又是一件事情，训练的网络结构和测试的网络结构不一样，就常识来说，训练和推理它网络结构得相同。要不然网络结构怎样去走呢。

- 在训练过程中将一个整体模块分割为多个相同或不同的模块分支。 
-  在推理过程中将多个分支集成到完全等价的模块（RepConv）。 
-  减少参数数量，加快推理速度，更加省内存。

实际做法：

- 卷积和BN的合并（数学方式） 
-  全部转成3×3的卷积。 
-  根据卷积可加性，多个卷积核再合并。

![img](https://developer.qcloudimg.com/http-save/yehe-11162134/b7c6503445cfee1a16cd98b7ebfb4758.png)

![img](https://i-blog.csdnimg.cn/blog_migrate/298695691b1239bb521cca9da9c80863.png#pic_center)



对应配置文件：

```
# FPN-PAN结构
[-1, 1, SPPCSPC, [512]], # 51

 [-1, 1, Conv, [256, 1, 1]],
 [-1, 1, nn.Upsample, [None, 2, 'nearest']],
 [37, 1, Conv, [256, 1, 1]], # route backbone P4  P4经过卷积后和P5的上采样拼接
 [[-1, -2], 1, Concat, [1]],
 
 [-1, 1, Conv, [256, 1, 1]], # elan模块
 [-2, 1, Conv, [256, 1, 1]],
 [-1, 1, Conv, [128, 3, 1]],
 [-1, 1, Conv, [128, 3, 1]],
 [-1, 1, Conv, [128, 3, 1]],
 [-1, 1, Conv, [128, 3, 1]],
 [[-1, -2, -3, -4, -5, -6], 1, Concat, [1]],
 [-1, 1, Conv, [256, 1, 1]], # 63
 
 [-1, 1, Conv, [128, 1, 1]],
 [-1, 1, nn.Upsample, [None, 2, 'nearest']],
 [24, 1, Conv, [128, 1, 1]], # route backbone P3  P3经过卷积后和先前的特征拼接
 [[-1, -2], 1, Concat, [1]],
 
 ...
 
 
 # detecthead结构
    [75, 1, RepConv, [256, 3, 1]],
   [88, 1, RepConv, [512, 3, 1]],
   [101, 1, RepConv, [1024, 3, 1]],

   [[102,103,104], 1, Detect, [nc, anchors]],   # Detect(P3, P4, P5)
```



### 损失函数

- 回归损失：CIoU 损失（比 IoU 损失更精准，考虑目标重叠度、中心点距离、宽高比）→ 计算正样本预测框与 GT 框的位置 / 形状误差；
- 分类损失：交叉熵损失（CE Loss）或 Focal Loss→ 计算正样本预测类别与真实类别的误差（Focal Loss 缓解类别不平衡）；
- 置信度损失：二元交叉熵损失（BCE Loss）→ 正样本标签 = 1，负样本标签 = 0，计算预测置信度与真实标签的误差。



### 标签分配

标签分配指的是将图片的标注框(标签)，也叫做真实框Ground Truth，和预测的预测值给对应起来，便于进一步求损失值。





#### 算法流程：

YOLOv5
Step1：Autoanchor策略，获得数据集最佳匹配的9个anchor（可选）

Step2：根据GT框与anchor的宽高比，过滤掉不合适的anchor
Step3：选择GT框的中心网格以及最邻近的2个邻域网格作为正样本筛选区域（辅助头则选择周围4个邻域网格）

YOLOX
Step4：计算GT框与正样本IOU并从大到小排序，选取前10个值进行求和（P6前20个），并取整作为当前GT框的K值（Dynamic K）
Step5：根据损失函数计算每个GT框和候选anchor的损失（Reg Loss和Cls Loss），保留损失最小的前K个
Step6：去掉同一个anchor被分配到多个GT框的情况



step1是数据预处理操作，为三个检测头（对应小、中、大目标）各分配 3 个 anchor 尺寸（共 9 个），使 anchors 尽可能贴合数据集中目标的真实宽高分布，提升初始匹配质量。（通过遍历所有GT获取最佳预设尺寸）

**步骤：**

1. **提取所有 ground truth bbox 的宽高**
   从训练集所有图像的标注文件（如 COCO 的 `annotations.json`）中，收集所有真实框的 *w*,*h* （单位：像素，通常归一化到 [0, 1] 或保持原始尺寸）。

2. 运行 k-means 聚类（k = 9）

   - 使用 

     IoU 距离

     （而非欧氏距离！），因为 bbox 匹配看的是重叠度。 

     距离度量定义为：

     *d*(box,anchor)=1−IoU(box,anchor)

   - 初始化 9 个聚类中心（可用 k-means++）；

   - 迭代：将每个 gt bbox 分配给 IoU 最大的 anchor；更新 anchor 为该簇 bbox 的 *centroid（注意：不是均值！需用 IoU 最优代表）*，常用做法是取该簇中与均值 bbox IoU 最大的那个作为新中心。

3. 按尺度排序分配给三个检测头

   - 将 9 个 anchors 按面积 *w*×*h* 排序；
   - 前 3 个（最小）→ stride=8（大特征图，检测小目标）
     中 3 个 → stride=16
     后 3 个（最大）→ stride=32（小特征图，检测大目标）

> 📌 示例（YOLOv5/v7 COCO 默认 anchors）：
>
> anchors = [
>
>   [12, 16, 19, 36, 40, 28],      # stride=8
>
>   [36, 75, 76, 55, 72, 146],     # stride=16
>
>   [142, 110, 192, 243, 459, 401] # stride=32
>
> ]
>
> （每两个数表示一组宽高）



step2初步筛掉形状差异太大的 anchor，防止强行把“瘦高 GT”分配给“扁平 anchor”导致回归难度爆炸；也把数量级先筛一遍，后面再精细筛选。

对于每个真实框，我们计算它与所有锚点的匹配程度，通常使用宽高比作为衡量标准。具体步骤如下：

1. **计算真实框与锚点的宽高比**：

   $ratio=\frac{min⁡(w_{gt},h_{gt})}{min⁡(w_{anchor},h_{anchor})}$

   或者更常见的，计算宽高比的相似性：

   $ratio_wh=\frac{w_{gt}}{w_{anchor}}/\frac{h_{gt}}{h_{anchor}}$

2. **设置阈值**：通常设定一个阈值（例如，4.0），如果真实框与锚点的宽高比小于阈值，则认为这个锚点可以用于预测该真实框。

3. **选择匹配的锚点**：对于每个真实框，我们选择与其宽高比最接近的锚点（即比值最接近1的锚点）作为正样本。同时，YOLOv7也允许一个真实框匹配多个锚点（例如，同一个真实框可以匹配三个尺度的锚点中的多个）。



v7的标签分配策略集成V5和YOLOX，同时引入额外的辅助头来指导标签分配，从而提高模型的性能。（有博文说该举措涨点不多，只有0.3个点）

![img](https://developer.qcloudimg.com/http-save/yehe-11162134/5d69eb9dad92bfe4683426056f08189b.png)

aux head更关注于recall，而lead head从aux head中精准筛选出样本。

在使用lead head和auxiliary head一起优化模型的时候，auxiliary head的正样本是较为“粗糙的“，主要是通过放宽正样本分配过程的约束来获得更多的正样本。lead head中的一个anchor如果匹配上ground truth，则分配3个正样本，而同样的情况下auxiliary head分配5个。

lead head中将top10个样本IOU求和取整，而auxiliary head中取top20。

auxiliary head的学习能力不如lead head强，为了避免丢失需要学习的信息，将重点优化auxiliary head的召回率。

而lead head可以从高recall的结果中筛选出高精度的结果作为最终输出。lead head和auxiliary head的损失函数权重设置为 4:1。

注：YOLOv7 引入「辅助检测头（Auxiliary Head）」，核心作用是**在训练阶段提供额外监督信号，缓解梯度消失，帮助主检测头（Lead Head）更快收敛、学习更鲁棒的特征**，推理阶段仅用主检测头输出结果（辅助头不参与推理）。

#### 1. 辅助头的输出内容

辅助头与主检测头结构完全一致，输出 3 类核心信息（和主检测头输出格式相同）：

- 回归分支：预测锚框的偏移量（dx, dy, dw, dh）→ 用于修正锚框位置和尺寸；
- 分类分支：预测目标类别概率（如 COCO 数据集 80 类）→ 判定目标类别；
- 置信度分支：预测 “该框是目标而非背景” 的置信度→ 筛选有效预测框。

简单说：辅助头就是一个 “备用检测头”，训练时和主检测头一起输出预测结果，推理时被 “关闭”。

#### 2. 辅助头的损失计算

辅助头的损失计算逻辑与主检测头完全一致，且**共享标签分配结果**（即正负样本划分和主检测头相同）

YOLOv7 的总训练损失是「主检测头损失 + 辅助检测头损失」的加权和，公式为：`Total Loss = Lead_Loss + λ × Aux_Loss`



![img](https://i-blog.csdnimg.cn/blog_migrate/f0eeed8ee6bb7ad023eb37ccb53d51da.png#pic_center)





### 检测结果

**原始网络输出（即检测头的未解码输出）** 是：

![image](https://img2024.cnblogs.com/blog/3147612/202501/3147612-20250118135725335-1257570824.png)

浅层特征图(80×80)适合用于检测小物体，因为其下采样的程度较小，可以保留更多的细节信息，可以想象一个很小的东西，如果下采样深度过高，它的信息可能就和巨多像素混在一起而丢失，因此较浅的下采样可以更好的留住小物体。深层特征图(20×20)适合用于检测大物体，因为其下采样的程度较大，对于大物体所占像素更多，若想获取其全部信息就需要采样更多的像素，因此可以更好的捕捉到大物体的特征。



对于每个 anchor（或每个预测分支），每个 grid cell 预测：

[ *$t\_x$*, *$t\_y$*, *$t\_w$*,$t\_h$, obj_conf, cls1,…,cls*C* ]



其中：

-  *$t\_x$*, *$t\_y$* ：**相对于当前 grid cell 左上角的偏移量（未归一化坐标，但范围在 0~1 附近）**
- *$t\_w$*,$t\_h$：**相对于(预设) anchor 宽高的缩放系数（对数空间）**
- obj_conf：objectness 置信度
- cls：类别置信度（通常 sigmoid 或 softmax 处理）





#### 解码

由预测头部分我们可以获得三个特征层的预测结果，以COCO为例，shape分别为(N,20,20,255)，(N,40,40,255)，(N,80,80,255)的数据。

其中的85可以拆分成4+1+80。
前4个参数用于判断每一个特征点的回归参数，回归参数调整后可以获得预测框；
第5个参数用于判断每一个特征点是否包含物体；
最后80个参数用于判断每一个特征点所包含的物体种类。



获取预测框位置信息，具体公式如下：

![image](https://img2024.cnblogs.com/blog/3147612/202501/3147612-20250118135848745-3094700.png)

其中tx,ty以及tw,th为预测数据也就是原始的输出数据，cx,cy为grid cell在原图中的左上角坐标，pw,ph为anchorbox的宽高，bx,by,bw,bh即为物体在原图中的坐标信息。其中σ()函数代表对数据进行sigmoid操作。





#### 得分筛选与NMS

得分筛选就是筛选出得分满足confidence置信度的预测框。

得分筛选与非极大抑制的过程可以概括如下：

1、找出该图片中得分大于门限函数的框。在进行重合框筛选前就进行得分的筛选可以大幅度减少框的数量。
2、对种类进行循环，非极大抑制的作用是筛选出一定区域内属于同一种类得分最大的框，对种类进行循环可以帮助我们对每一个类分别进行非极大抑制。
3、根据得分对该种类进行从大到小排序。
4、每次取出得分最大的框，计算其与其它所有预测框的重合程度，重合程度过大的则剔除。







## YOLOV8

### 概述

无锚框

预测锚框边界到中心点之间的距离，而不是预设锚框尺寸

### 参考博客

1.[Yolov 8源码超详细逐行解读+ 网络结构细讲(自我用的小白笔记)-CSDN博客](https://blog.csdn.net/chenhaogu/article/details/131647758)

2.[YOLOv8 原理和实现全解析 — MMYOLO 0.6.0 文档](https://mmyolo.readthedocs.io/zh-cn/latest/recommended_topics/algorithm_descriptions/yolov8_description.html)

3.[YOLOV8系列模型部署与后处理 | Alex Tech Garden](https://alexw914.github.io/2025/08/03/YOLOV8系列模型部署与后处理/#3-NMS)

标签分配：

4.[(29 封私信 / 80 条消息) YOLOv8 目标检测模型 原理解析 - 知乎](https://zhuanlan.zhihu.com/p/722076107)

5.[(29 封私信 / 80 条消息) YOLOv8-训练流程-正负样本分配 - 知乎](https://zhuanlan.zhihu.com/p/633094573)



### 模型结构

![YOLOv8-P5_structure](https://user-images.githubusercontent.com/27466624/222869864-1955f054-aa6d-4a80-aed3-92f30af28849.jpg)

#### 主干网络backbone



#### Head

同样可以分为Neck（FPN-PAN）和DetectHead



**DetectHead**

包含两个头：分类头+box头

输出`NC+4*regmax`维向量

原代码设置：

self.reg_max = 16  *# DFL channels (ch[0] // 16 to scale 4/8/12/16/20 for n/s/m/l/x)*

检测头只预测类别和box，取消objectness分支，由分类头确定置信度

regmax有4组，分别预测边界框的 4 个坐标l、t、b、r（左上右下），每个坐标值都有 16 个离散点进行预测。对离散化的坐标 l,t,r,b 应用 softmax，得到每个离散值的概率分布。例如，对于l，softmax 会输出一个 16 维的向量，**每个元素代表该离散值的概率**。参考博客：[(29 封私信 / 80 条消息) YOLOv8 目标检测模型 原理解析 - 知乎](https://zhuanlan.zhihu.com/p/722076107)

![img](https://pic1.zhimg.com/v2-1c702ab8cd66d309d8462b9522bf5028_1440w.jpg)

解码方法：首先对16个一组的坐标值进行计算**Softmax**，与regmax的16个索引值相乘后相加，得到最终的坐标值(这里计算的是一个期望值)。得到输出后，根据中心点计算坐标回归，这里的中心点是每个预测方格的中心点。

```python
C, H, W = output.shape                           # [66, 160, 160]
output = output.reshape(C, -1).T                 # [160x160, 66]                                 
bbox_dfl = output[:, :NUM_BINS * 4].reshape(-1, 4, NUM_BINS)  #[160x160, 4, 16]

prob = softmax(bbox_dfl, axis=-1)                  
proj = np.arange(NUM_BINS, dtype=np.float32)      # [0, 1, 2, ... 15]        
bbox = np.sum(prob * proj, axis=-1) * stride      # [160x160, 4]

y, x = np.divmod(np.arange(H * W), W)
grid_x = (x * stride + stride / 2)
grid_y = (y * stride + stride / 2)                # 构造中心点坐标

x1 = (grid_x - bbox[:, 0])                        # 坐标回归
y1 = (grid_y - bbox[:, 1])
x2 = (grid_x + bbox[:, 2])
y2 = (grid_y + bbox[:, 3])

```



类别预测较为简单，对每个预测类别输出做sigmoid即可, 即可得到最终的置信度。(部分网络可能使用Softmax计算，用于输出**不可能同时满足两个类别**的情况)，类别大于conf_thresh阈值需要使用NMS进行进一步过滤。



### 损失函数

#### DFL损失(Distribution Focal Loss)

用于计算锚框边界分布损失

**离散概率分布建模**

DFL将连续坐标值建模为**离散区间上的概率分布**。假设坐标真实值为 y，将其离散化为 n 个点 {y0,y1,...,yn−1}，模型预测每个点的概率 P(yi)。

**2. 积分计算连续值**

通过加权求和（积分）得到最终预测坐标：

![img](https://i-blog.csdnimg.cn/direct/2d2119439d334c1e8430c1aea846be40.png)

**示例**：

![img](https://i-blog.csdnimg.cn/direct/f6f0b72ca825491987f5324a95e5ebed.png)

#### CIOU损失



#### 分类损失



### 标签分配

TaskAlignedAssigner策略

匹配策略简单总结为：根据分类与回归的分数加权的分数选择正样本。

（任务对齐度量，换做其他任务可能是其他分数的加权分数进行度量）

第一步：筛选正样本

1.对每个真实框GT，以align_metric匹配程度排序，选取[topK](https://so.csdn.net/so/search?q=topK&spm=1001.2101.3001.7020)个预测框作为正样本。

首先筛选锚点(特征图grid的坐标中心点)落在gt_box中, 得到mask_in_gt((Tensor): shape(b, n_boxes, h*w)), 其中1代表锚点落在gt_box中, 0表示锚点未落在gt_box中。如下图所示

![img](https://pica.zhimg.com/v2-8339e82960acb15fa061a466924b40c4_1440w.jpg)



第二步：计算匹配程度

1.计算对齐分数

公式如下：
$$
align\_{metric} =s^\alpha * u^\beta
$$
s是预测类别分值，u是预测框和真实框的ciou值，‘ α ‘ 和‘ β ‘ 为权重超参数，两者相乘就可以衡量匹配程度，当分类的分值越高且ciou越高时，align_metric的值就越接近于1,此时预测框就与真实框越匹配，就越符合正样本的标准。
该度量结合了分类置信度和IoU值，用于全面衡量锚点的质量。

下图左边是任务对齐度量，右边是IoU重叠度量。

![img](https://pic3.zhimg.com/v2-fb14546ff280d4b803527a0866d9f020_1440w.jpg)

第三步：

通过 select_topk_candidates 函数从所有候选的锚点中，选择与每个GT框 对齐度量 最高的前K个锚点，并为这些锚点生成相应的掩码 mask_topk。

![img](https://pic3.zhimg.com/v2-f24f39b6ef49eb76204cec57cb05173a_1440w.jpg)



第四步：

将前三步生成的所有掩码进行合并，得到最终的正样本Mask。

同时对一个预测框与多个真实框匹配测情况进行处理，保留ciou值最大的真实框。

如下图所示，左右两个真实框有一个预测框重合，我们保留iou更高的，舍去绿色的

![img](https://pica.zhimg.com/v2-fa1d73b5d43a3af6605165acc23b1550_1440w.jpg)

代码：[yolov8正负样本标签分配策略代码解析_yolov8正负样本分配-CSDN博客](https://blog.csdn.net/shilichangtin/article/details/135126709)



### 疑问：

1.不管负样本？负样本是否参与损失计算

2.每个尺度特征图都会分配gt吗，或者预测？



### 输出解码

将4*regmax 的dfl预测转为 4维的box信息，其余和前几代做法相同

![head](https://user-images.githubusercontent.com/17425982/212816206-33815716-3c12-49a2-9c37-0bd85f941bec.png)





## 超参数

| 序号 | 参数                | 值     | 含义与解释                                                   |
| ---- | ------------------- | ------ | ------------------------------------------------------------ |
| 1    | `--model`/`--cfg`   | YOLOv7 | 指定模型结构文件（如`yolov7.yaml`）。 ⚠️ 注意：不是字符串`"YOLOV7"`，而是`.yaml`配置路径；权重初始化通常用`--weights yolov7.pt`。 |
| 2    | `--epochs`          | 300    | 总训练轮数（遍历全部训练集的次数）。 ✅ 300 对中等规模数据集较合理；若数据少（如 <1k 图像），可能过拟合 → 可减至 100~200；若用`--resume`续训，**不会重置 epoch 计数**。 |
| 3    | `--batch-size`      | 16     | 每批送入 GPU 的图像数。 ✅ 16 是 16GB 显存（如 RTX 3090）较稳妥值；若 OOM（显存溢出），可设为 8 或用`--batch-size -1`（自动缩放至最大）。 📌*注：batch 越大，训练越稳，但显存压力越大*。 |
| 4    | `--img-size`        | 640    | 输入图像被 resize 到`640×640`（保持宽高比，短边=640，长边≤640 + padding）。 ✅ 640 是 YOLOv7 默认值，平衡速度与精度；小目标多？可试 800；追求速度？可试 416。 |
| 5    | `--rect`            | True   | **矩形训练（Rectangular Training）**： → 同 batch 内图像按**最大公共尺寸**对齐（减少 padding），提升 GPU 利用率 & 训练速度（约快 30%）。 ⚠️ 推理时仍需标准 square 输入（如 640×640）。 |
| 6    | `--resume`          | True   | 从最近保存的 checkpoint（如`last.pt`）**继续训练**，**自动恢复 epoch、optimizer 状态等**。 ✅ 中断后续训必备；首次训练应为`False`。 |
| 7    | `--nosave`          | False  | 是否**保存 checkpoint**： `False`→ 保存`best.pt`和`last.pt`； `True`→ 只保存最终模型（省磁盘空间）。 ✅ 建议保持`False`，方便中断恢复或选最佳模型。 |
| 8    | `--notest`          | False  | 是否**跳过每个 epoch 后的 val 阶段**： `False`→ 每 epoch 都 val（得 mAP）； `True`→ 只最后 val 一次（快，但不知中间性能）。 ✅ 小数据集建议`False`（监控过拟合）；大数据集可设`True`加速。 |
| 9    | `--noautoanchor`    | True   | **禁用自动 anchor 优化**： YOLO 默认用 k-means 聚类训练集 bbox 得 anchor； `True`→**强制使用 cfg 中预设 anchor**（如 COCO 的）。 ✅ 若你数据集 bbox 分布与 COCO 差异大（如全是细长目标），建议设为`False`让其自动优化！否则可能掉点。 |
| 10   | `--evolve`          | False  | 是否**超参进化（遗传算法调参）**： `True`→ 用多轮训练搜索最优超参（耗时 10x+）； `False`→ 用当前超参训练。 ✅ 通常`False`；除非你有大量算力 & 时间。 |
| 11   | `--cos-lr`          | 0.001  | ❗ 此处有误：`cos-lr`是**布尔开关**，控制是否用**余弦退火学习率**； 你写的`0.001`应该是`--lr0`（初始学习率）。 ✅ 推荐： `bash<br>--cos-lr # 启用余弦退火（默认）<br>--lr0 0.01 # 初始 lr（batch=64 时）；batch=16 时建议 lr0=0.01×(16/64)=0.0025<br>` |
| 12   | `--label-smoothing` | 0.1    | 标签平滑（Label Smoothing）：防止模型过拟合到 one-hot 标签。 → 将真实标签`1`→`1 - ε`，其余`0`→`ε/(n-1)`（ε=0.1）。 ✅ 对小数据集 & 小类别有益（如你提到的样本不均衡），推荐 0.0~0.2。 |
| 13   | `--patience`        | 30     | **早停耐心值（Early Stopping）**： → 若 val 损失连续 30 轮未改善，则提前停止训练。 ✅ 30 较合理；数据少可减至 10~15；配合`--notest False`使用。 |
| 14   | `--multi-scale`     | ±50%   | 多尺度训练：每 batch 随机缩放图像尺寸在`[0.5×img_size, 1.5×img_size]`（即 320~960）。 ✅**增强泛化性**，尤其对尺度变化大的目标（如远近车辆）；但增加训练时间 ~20%。 |
| 15   | `--single-cls`      | 0      | 是否将**所有类别视为一类**训练（用于迁移学习或二分类场景）。 `0`/`False`→ 多分类；`1`/`True`→ 合并为 single class。 ✅ 你做多类别分类（如设备识别），应为`0`或省略。 |

解释：

一、`--evolve`：搜索**最佳训练超参数**（**训练前 / 训练间**）

🔍 本质：

> **离线、全局、试错式优化**：找一套能让模型**最终收敛效果最好**的超参数组合。 

🎯 目标：

- 提升 **最终模型的 mAP / Recall / Precision**
- 找到对**你的特定数据集**最鲁棒的训练配置

🔧 它调哪些参数？（举几个关键例子）

| 类别            | 参数                                   | 作用                     |
| --------------- | -------------------------------------- | ------------------------ |
| **优化器**      | `lr0`,`lrf`,`momentum`,`weight_decay`  | 控制学习快慢 & 稳定性    |
| **损失权重**    | `box`,`cls`,`obj`                      | 平衡定位/分类/置信度损失 |
| **数据增强**    | `hsv_h/s/v`,`degrees`,`mosaic`,`mixup` | 控制图像扰动强度         |
| **Anchor 相关** | `anchor_t`                             | 控制正负样本匹配阈值     |

✅ **类比**：
像选“厨师 + 配方 + 火候”——`evolve` 尝试 300 个厨师，每人用不同配方做一锅汤（完整训练），最后选最好喝的那锅的配方。

**二、`EMA`（Exponential Moving Average）：平滑模型权重（训练阶段）**

🔍 本质：

> **在线、局部、稳定化技术**：不改变超参数，而是**让模型权重更稳定、泛化更好**，ema不参与损失计算或者优化，只是单纯平滑权重。 使用ema相关权重通常表现更好，更具备泛化性

📌 官方 YOLOv7 默认开启（代码中 `--ema` 通常为 True，无需显式加）

🔧 原理（极简版）：

训练时维护两套权重：

- `model`：**实时权重**，每步 SGD 更新，震荡大；

- `ema_model`：**影子权重（EMA）**，按公式缓慢更新：

  $ema\_weight_t = β*⋅ema\_weight_t*−1+(1− β )⋅model\_weight_t $

  其中 *β* 接近 1（如 0.999），所以 EMA 权重是**历史权重的指数加权平均**。

代码：

```python
class ModelEMA:
    def __init__(self, model, decay=0.9999):
        self.ema = deepcopy(model).eval()  # 影子模型，初始 = model
        self.updates = 0
        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # warm-up decay

    def update(self, model):
        with torch.no_grad():
            self.updates += 1
            d = self.decay(self.updates)      # 当前衰减率 β，初期小，后期趋近 0.9999
            msd = model.state_dict()          # 实时模型权重 dict
            for k, v in self.ema.state_dict().items():
                if v.dtype.is_floating_point:
                    v *= d                    # ema = ema * β
                    v += (1 - d) * msd[k].detach()  # + (1-β) * model_weight
```

有公式可知衰减率初期小，后期大

```
Weight Value
  ↑
  |                          ● model (实时，震荡大)
  |                        ↗
  |                      ↗
  |        EMA (平滑) ●↗
  |                ↗
  |              ↗
  |  ● (初始 = model 初始值)
  +--------------------------------→ Training Steps
   0
```

- 起点重合（✅ 初始化 = model 初始权重）；
- 前期 EMA 追得快（warm-up decay 小）；
- 后期 EMA 走得稳（decay 大）。